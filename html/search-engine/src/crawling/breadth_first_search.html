<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>search-engine.src.crawling.breadth_first_search API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>search-engine.src.crawling.breadth_first_search</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from src.database.database import Database
from src.crawling.page_content import PageContent
from src.crawling.util import Util
from datetime import datetime
from urllib.parse import urljoin
import bs4
import threading
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures.thread
import queue
import time
import re


class BreadthFirstSearch:
    &#34;&#34;&#34;Kelas yang digunakan untuk melakukan crawling dengan metode Breadth First Search.&#34;&#34;&#34;

    def __init__(self, crawl_id, url_queue, visited_urls, duration_sec, max_threads):
        self.crawl_id = crawl_id
        self.url_queue = url_queue
        self.visited_urls = visited_urls
        self.duration_sec = duration_sec
        self.max_threads = max_threads
        self.db = Database()
        self.page_content = PageContent()
        self.util = Util()
        self.lock = threading.Lock()
        self.start_time = time.time()
        self.list_urls = []

    def run(self):
        &#34;&#34;&#34;Fungsi utama yang berfungsi untuk menjalankan proses crawling BFS.&#34;&#34;&#34;
        executor = ThreadPoolExecutor(max_workers=self.max_threads)

        futures = []
        while True:
            try:
                time_now = time.time() - self.start_time
                time_now_int = int(time_now)
                if time_now_int &gt;= self.duration_sec:
                    print(&#34;Stopped because exceeded time limit...&#34;)
                    break
                target_url = self.url_queue.get(timeout=60)
                if target_url not in self.visited_urls:
                    self.visited_urls.append(target_url)
                    futures.append(executor.submit(self.scrape_page, target_url))
            except queue.Empty:
                if self.util.running_thread_count(futures) &gt; 0:
                    continue
                else:
                    print(&#34;Stopped because empty queue...&#34;)
                    break
            except KeyboardInterrupt:
                print(&#34;Stopped because keyboard interrupt...&#34;)
                break
            except Exception as e:
                print(e)
                continue

        executor._threads.clear()
        concurrent.futures.thread._threads_queues.clear()

    def scrape_page(self, url):
        &#34;&#34;&#34;Fungsi untuk menyimpan konten yang ada pada suatu halaman ke database.&#34;&#34;&#34;
        try:
            response = self.util.get_page(url)
            if response and response.status_code == 200:
                db_connection = self.db.connect()
                self.lock.acquire()
                now = datetime.now()
                print(url, &#34;| BFS |&#34;, now.strftime(&#34;%d/%m/%Y %H:%M:%S&#34;))
                self.lock.release()
                soup = bs4.BeautifulSoup(response.text, &#34;html.parser&#34;)
                title = soup.title.string
                article_html5 = soup.find(&#34;article&#34;)
                if article_html5 is None:
                    # extract text content from html4
                    html5 = 0
                    texts = soup.find(&#34;body&#34;).findAll(text=True)
                    visible_texts = filter(self.tag_visible, texts)
                    text = &#34; &#34;.join(t.strip() for t in visible_texts)
                    text = text.lstrip().rstrip()
                    text = text.split(&#34;,&#34;)
                    clean_text = &#34;&#34;
                    for sen in text:
                        if sen:
                            sen = sen.rstrip().lstrip()
                            clean_text += sen + &#34;,&#34;
                    complete_text = clean_text
                else:
                    # extract text content from html5
                    html5 = 1
                    texts = article_html5.findAll(text=True)
                    visible_texts = filter(self.tag_visible, texts)
                    text = &#34; &#34;.join(t.strip() for t in visible_texts)
                    text = text.lstrip().rstrip()
                    text = text.split(&#34;,&#34;)
                    clean_text = &#34;&#34;
                    for sen in text:
                        if sen:
                            sen = sen.rstrip().lstrip()
                            clean_text += sen + &#34;,&#34;
                    complete_text = clean_text

                # get meta description
                description = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;description&#34;})
                if description is None:
                    description = &#34;-&#34;
                else:
                    description = description.get(&#34;content&#34;)

                # get meta keywords
                keywords = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;keywords&#34;})
                if keywords is None:
                    keywords = &#34;-&#34;
                else:
                    keywords = keywords.get(&#34;content&#34;)

                # isHotURL
                hot_link = 0

                # check if the page information already exist
                if not self.db.check_value_in_table(db_connection, &#34;page_information&#34;, &#34;url&#34;, url):
                    self.page_content.insert_page_information(
                        db_connection,
                        url,
                        self.crawl_id,
                        html5,
                        title,
                        description,
                        keywords,
                        complete_text,
                        hot_link,
                        &#34;BFS crawling&#34;,
                    )
                else:
                    self.db.close_connection(db_connection)
                    return

                # extract style
                for style in soup.findAll(&#34;style&#34;):
                    self.page_content.insert_page_style(db_connection, url, style)

                # extract script
                for script in soup.findAll(&#34;script&#34;):
                    self.page_content.insert_page_script(db_connection, url, script)

                # extract lists
                for lists in soup.findAll(&#34;li&#34;):
                    self.page_content.insert_page_list(db_connection, url, lists)

                # extract forms
                for form in soup.findAll(&#34;form&#34;):
                    self.page_content.insert_page_form(db_connection, url, form)

                # extract tables
                for table in soup.findAll(&#34;table&#34;):
                    self.page_content.insert_page_table(db_connection, url, table)

                # extract images
                for image in soup.findAll(&#34;img&#34;):
                    self.page_content.insert_page_image(db_connection, url, image)

                # extract outgoing link
                links = soup.findAll(&#34;a&#34;, href=True)
                for i in links:
                    # Complete relative URLs and strip trailing slash
                    complete_url = urljoin(url, i[&#34;href&#34;]).rstrip(&#34;/&#34;)

                    # self.list_urls.append(complete_url) # Disable Modified Similarity Based Crawler
                    self.page_content.insert_page_linking(db_connection, self.crawl_id, url, complete_url)

                    self.lock.acquire()
                    if self.util.is_valid_url(complete_url) and complete_url not in self.visited_urls:
                        self.url_queue.put(complete_url)
                    self.lock.release()

                self.db.close_connection(db_connection)
                return
            return
        except Exception as e:
            print(e, &#34;~ Error in thread&#34;)
            return

    def tag_visible(self, element):
        &#34;&#34;&#34;Fungsi untuk merapihkan konten teks.&#34;&#34;&#34;
        if element.parent.name in [&#34;style&#34;, &#34;script&#34;, &#34;head&#34;, &#34;title&#34;, &#34;meta&#34;, &#34;[document]&#34;]:
            return False
        if isinstance(element, bs4.element.Comment):
            return False
        if re.match(r&#34;[\n]+&#34;, str(element)):
            return False
        return True</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="search-engine.src.crawling.breadth_first_search.BreadthFirstSearch"><code class="flex name class">
<span>class <span class="ident">BreadthFirstSearch</span></span>
<span>(</span><span>crawl_id, url_queue, visited_urls, duration_sec, max_threads)</span>
</code></dt>
<dd>
<div class="desc"><p>Kelas yang digunakan untuk melakukan crawling dengan metode Breadth First Search.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BreadthFirstSearch:
    &#34;&#34;&#34;Kelas yang digunakan untuk melakukan crawling dengan metode Breadth First Search.&#34;&#34;&#34;

    def __init__(self, crawl_id, url_queue, visited_urls, duration_sec, max_threads):
        self.crawl_id = crawl_id
        self.url_queue = url_queue
        self.visited_urls = visited_urls
        self.duration_sec = duration_sec
        self.max_threads = max_threads
        self.db = Database()
        self.page_content = PageContent()
        self.util = Util()
        self.lock = threading.Lock()
        self.start_time = time.time()
        self.list_urls = []

    def run(self):
        &#34;&#34;&#34;Fungsi utama yang berfungsi untuk menjalankan proses crawling BFS.&#34;&#34;&#34;
        executor = ThreadPoolExecutor(max_workers=self.max_threads)

        futures = []
        while True:
            try:
                time_now = time.time() - self.start_time
                time_now_int = int(time_now)
                if time_now_int &gt;= self.duration_sec:
                    print(&#34;Stopped because exceeded time limit...&#34;)
                    break
                target_url = self.url_queue.get(timeout=60)
                if target_url not in self.visited_urls:
                    self.visited_urls.append(target_url)
                    futures.append(executor.submit(self.scrape_page, target_url))
            except queue.Empty:
                if self.util.running_thread_count(futures) &gt; 0:
                    continue
                else:
                    print(&#34;Stopped because empty queue...&#34;)
                    break
            except KeyboardInterrupt:
                print(&#34;Stopped because keyboard interrupt...&#34;)
                break
            except Exception as e:
                print(e)
                continue

        executor._threads.clear()
        concurrent.futures.thread._threads_queues.clear()

    def scrape_page(self, url):
        &#34;&#34;&#34;Fungsi untuk menyimpan konten yang ada pada suatu halaman ke database.&#34;&#34;&#34;
        try:
            response = self.util.get_page(url)
            if response and response.status_code == 200:
                db_connection = self.db.connect()
                self.lock.acquire()
                now = datetime.now()
                print(url, &#34;| BFS |&#34;, now.strftime(&#34;%d/%m/%Y %H:%M:%S&#34;))
                self.lock.release()
                soup = bs4.BeautifulSoup(response.text, &#34;html.parser&#34;)
                title = soup.title.string
                article_html5 = soup.find(&#34;article&#34;)
                if article_html5 is None:
                    # extract text content from html4
                    html5 = 0
                    texts = soup.find(&#34;body&#34;).findAll(text=True)
                    visible_texts = filter(self.tag_visible, texts)
                    text = &#34; &#34;.join(t.strip() for t in visible_texts)
                    text = text.lstrip().rstrip()
                    text = text.split(&#34;,&#34;)
                    clean_text = &#34;&#34;
                    for sen in text:
                        if sen:
                            sen = sen.rstrip().lstrip()
                            clean_text += sen + &#34;,&#34;
                    complete_text = clean_text
                else:
                    # extract text content from html5
                    html5 = 1
                    texts = article_html5.findAll(text=True)
                    visible_texts = filter(self.tag_visible, texts)
                    text = &#34; &#34;.join(t.strip() for t in visible_texts)
                    text = text.lstrip().rstrip()
                    text = text.split(&#34;,&#34;)
                    clean_text = &#34;&#34;
                    for sen in text:
                        if sen:
                            sen = sen.rstrip().lstrip()
                            clean_text += sen + &#34;,&#34;
                    complete_text = clean_text

                # get meta description
                description = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;description&#34;})
                if description is None:
                    description = &#34;-&#34;
                else:
                    description = description.get(&#34;content&#34;)

                # get meta keywords
                keywords = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;keywords&#34;})
                if keywords is None:
                    keywords = &#34;-&#34;
                else:
                    keywords = keywords.get(&#34;content&#34;)

                # isHotURL
                hot_link = 0

                # check if the page information already exist
                if not self.db.check_value_in_table(db_connection, &#34;page_information&#34;, &#34;url&#34;, url):
                    self.page_content.insert_page_information(
                        db_connection,
                        url,
                        self.crawl_id,
                        html5,
                        title,
                        description,
                        keywords,
                        complete_text,
                        hot_link,
                        &#34;BFS crawling&#34;,
                    )
                else:
                    self.db.close_connection(db_connection)
                    return

                # extract style
                for style in soup.findAll(&#34;style&#34;):
                    self.page_content.insert_page_style(db_connection, url, style)

                # extract script
                for script in soup.findAll(&#34;script&#34;):
                    self.page_content.insert_page_script(db_connection, url, script)

                # extract lists
                for lists in soup.findAll(&#34;li&#34;):
                    self.page_content.insert_page_list(db_connection, url, lists)

                # extract forms
                for form in soup.findAll(&#34;form&#34;):
                    self.page_content.insert_page_form(db_connection, url, form)

                # extract tables
                for table in soup.findAll(&#34;table&#34;):
                    self.page_content.insert_page_table(db_connection, url, table)

                # extract images
                for image in soup.findAll(&#34;img&#34;):
                    self.page_content.insert_page_image(db_connection, url, image)

                # extract outgoing link
                links = soup.findAll(&#34;a&#34;, href=True)
                for i in links:
                    # Complete relative URLs and strip trailing slash
                    complete_url = urljoin(url, i[&#34;href&#34;]).rstrip(&#34;/&#34;)

                    # self.list_urls.append(complete_url) # Disable Modified Similarity Based Crawler
                    self.page_content.insert_page_linking(db_connection, self.crawl_id, url, complete_url)

                    self.lock.acquire()
                    if self.util.is_valid_url(complete_url) and complete_url not in self.visited_urls:
                        self.url_queue.put(complete_url)
                    self.lock.release()

                self.db.close_connection(db_connection)
                return
            return
        except Exception as e:
            print(e, &#34;~ Error in thread&#34;)
            return

    def tag_visible(self, element):
        &#34;&#34;&#34;Fungsi untuk merapihkan konten teks.&#34;&#34;&#34;
        if element.parent.name in [&#34;style&#34;, &#34;script&#34;, &#34;head&#34;, &#34;title&#34;, &#34;meta&#34;, &#34;[document]&#34;]:
            return False
        if isinstance(element, bs4.element.Comment):
            return False
        if re.match(r&#34;[\n]+&#34;, str(element)):
            return False
        return True</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="search-engine.src.crawling.breadth_first_search.BreadthFirstSearch.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Fungsi utama yang berfungsi untuk menjalankan proses crawling BFS.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34;Fungsi utama yang berfungsi untuk menjalankan proses crawling BFS.&#34;&#34;&#34;
    executor = ThreadPoolExecutor(max_workers=self.max_threads)

    futures = []
    while True:
        try:
            time_now = time.time() - self.start_time
            time_now_int = int(time_now)
            if time_now_int &gt;= self.duration_sec:
                print(&#34;Stopped because exceeded time limit...&#34;)
                break
            target_url = self.url_queue.get(timeout=60)
            if target_url not in self.visited_urls:
                self.visited_urls.append(target_url)
                futures.append(executor.submit(self.scrape_page, target_url))
        except queue.Empty:
            if self.util.running_thread_count(futures) &gt; 0:
                continue
            else:
                print(&#34;Stopped because empty queue...&#34;)
                break
        except KeyboardInterrupt:
            print(&#34;Stopped because keyboard interrupt...&#34;)
            break
        except Exception as e:
            print(e)
            continue

    executor._threads.clear()
    concurrent.futures.thread._threads_queues.clear()</code></pre>
</details>
</dd>
<dt id="search-engine.src.crawling.breadth_first_search.BreadthFirstSearch.scrape_page"><code class="name flex">
<span>def <span class="ident">scrape_page</span></span>(<span>self, url)</span>
</code></dt>
<dd>
<div class="desc"><p>Fungsi untuk menyimpan konten yang ada pada suatu halaman ke database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scrape_page(self, url):
    &#34;&#34;&#34;Fungsi untuk menyimpan konten yang ada pada suatu halaman ke database.&#34;&#34;&#34;
    try:
        response = self.util.get_page(url)
        if response and response.status_code == 200:
            db_connection = self.db.connect()
            self.lock.acquire()
            now = datetime.now()
            print(url, &#34;| BFS |&#34;, now.strftime(&#34;%d/%m/%Y %H:%M:%S&#34;))
            self.lock.release()
            soup = bs4.BeautifulSoup(response.text, &#34;html.parser&#34;)
            title = soup.title.string
            article_html5 = soup.find(&#34;article&#34;)
            if article_html5 is None:
                # extract text content from html4
                html5 = 0
                texts = soup.find(&#34;body&#34;).findAll(text=True)
                visible_texts = filter(self.tag_visible, texts)
                text = &#34; &#34;.join(t.strip() for t in visible_texts)
                text = text.lstrip().rstrip()
                text = text.split(&#34;,&#34;)
                clean_text = &#34;&#34;
                for sen in text:
                    if sen:
                        sen = sen.rstrip().lstrip()
                        clean_text += sen + &#34;,&#34;
                complete_text = clean_text
            else:
                # extract text content from html5
                html5 = 1
                texts = article_html5.findAll(text=True)
                visible_texts = filter(self.tag_visible, texts)
                text = &#34; &#34;.join(t.strip() for t in visible_texts)
                text = text.lstrip().rstrip()
                text = text.split(&#34;,&#34;)
                clean_text = &#34;&#34;
                for sen in text:
                    if sen:
                        sen = sen.rstrip().lstrip()
                        clean_text += sen + &#34;,&#34;
                complete_text = clean_text

            # get meta description
            description = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;description&#34;})
            if description is None:
                description = &#34;-&#34;
            else:
                description = description.get(&#34;content&#34;)

            # get meta keywords
            keywords = soup.find(&#34;meta&#34;, attrs={&#34;name&#34;: &#34;keywords&#34;})
            if keywords is None:
                keywords = &#34;-&#34;
            else:
                keywords = keywords.get(&#34;content&#34;)

            # isHotURL
            hot_link = 0

            # check if the page information already exist
            if not self.db.check_value_in_table(db_connection, &#34;page_information&#34;, &#34;url&#34;, url):
                self.page_content.insert_page_information(
                    db_connection,
                    url,
                    self.crawl_id,
                    html5,
                    title,
                    description,
                    keywords,
                    complete_text,
                    hot_link,
                    &#34;BFS crawling&#34;,
                )
            else:
                self.db.close_connection(db_connection)
                return

            # extract style
            for style in soup.findAll(&#34;style&#34;):
                self.page_content.insert_page_style(db_connection, url, style)

            # extract script
            for script in soup.findAll(&#34;script&#34;):
                self.page_content.insert_page_script(db_connection, url, script)

            # extract lists
            for lists in soup.findAll(&#34;li&#34;):
                self.page_content.insert_page_list(db_connection, url, lists)

            # extract forms
            for form in soup.findAll(&#34;form&#34;):
                self.page_content.insert_page_form(db_connection, url, form)

            # extract tables
            for table in soup.findAll(&#34;table&#34;):
                self.page_content.insert_page_table(db_connection, url, table)

            # extract images
            for image in soup.findAll(&#34;img&#34;):
                self.page_content.insert_page_image(db_connection, url, image)

            # extract outgoing link
            links = soup.findAll(&#34;a&#34;, href=True)
            for i in links:
                # Complete relative URLs and strip trailing slash
                complete_url = urljoin(url, i[&#34;href&#34;]).rstrip(&#34;/&#34;)

                # self.list_urls.append(complete_url) # Disable Modified Similarity Based Crawler
                self.page_content.insert_page_linking(db_connection, self.crawl_id, url, complete_url)

                self.lock.acquire()
                if self.util.is_valid_url(complete_url) and complete_url not in self.visited_urls:
                    self.url_queue.put(complete_url)
                self.lock.release()

            self.db.close_connection(db_connection)
            return
        return
    except Exception as e:
        print(e, &#34;~ Error in thread&#34;)
        return</code></pre>
</details>
</dd>
<dt id="search-engine.src.crawling.breadth_first_search.BreadthFirstSearch.tag_visible"><code class="name flex">
<span>def <span class="ident">tag_visible</span></span>(<span>self, element)</span>
</code></dt>
<dd>
<div class="desc"><p>Fungsi untuk merapihkan konten teks.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tag_visible(self, element):
    &#34;&#34;&#34;Fungsi untuk merapihkan konten teks.&#34;&#34;&#34;
    if element.parent.name in [&#34;style&#34;, &#34;script&#34;, &#34;head&#34;, &#34;title&#34;, &#34;meta&#34;, &#34;[document]&#34;]:
        return False
    if isinstance(element, bs4.element.Comment):
        return False
    if re.match(r&#34;[\n]+&#34;, str(element)):
        return False
    return True</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="search-engine.src.crawling" href="index.html">search-engine.src.crawling</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="search-engine.src.crawling.breadth_first_search.BreadthFirstSearch" href="#search-engine.src.crawling.breadth_first_search.BreadthFirstSearch">BreadthFirstSearch</a></code></h4>
<ul class="">
<li><code><a title="search-engine.src.crawling.breadth_first_search.BreadthFirstSearch.run" href="#search-engine.src.crawling.breadth_first_search.BreadthFirstSearch.run">run</a></code></li>
<li><code><a title="search-engine.src.crawling.breadth_first_search.BreadthFirstSearch.scrape_page" href="#search-engine.src.crawling.breadth_first_search.BreadthFirstSearch.scrape_page">scrape_page</a></code></li>
<li><code><a title="search-engine.src.crawling.breadth_first_search.BreadthFirstSearch.tag_visible" href="#search-engine.src.crawling.breadth_first_search.BreadthFirstSearch.tag_visible">tag_visible</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>